{"cells":[{"cell_type":"markdown","metadata":{"id":"dj0IlhwS7BMf"},"source":["# Setting up the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W3Gyjgg5Vzjv"},"outputs":[],"source":["from google.colab import drive\n","import os\n","from google.colab import userdata\n","\n","# Step 1: Mount Google Drive\n","# Mount Google Drive to access your GitHub repository\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Step 2: Navigate to Your GitHub Repository\n","# Change to your repository's location in Google Drive\n","repo_path = \"/content/drive/MyDrive/colab_repos/Wav2Vec2-vs-HUbert\"  # Adjust to your repository path\n","os.chdir(repo_path)\n","\n","# Step 3: Set Git User Identity\n","# Configure Git with your username and email for committing\n","!git config --global user.name \"FilipLarsson12\"\n","!git config --global user.email \"hockeyfilip12@gmail.com\"\n","\n","# Step 4: Configure Git Remote\n","# Use the GitHub Personal Access Token from secrets for authentication\n","github_token = userdata.get(\"github_access_token\")  # Retrieve the secret\n","repo_url = f\"https://{github_token}@github.com/FilipLarsson12/Wav2Vec2-vs-HUbert.git\"\n","\n","# Set or update the Git remote\n","!git remote set-url origin {repo_url}\n","\n","# Step 5: Stage, Commit, and Push Changes\n","# Add the file(s) to the Git staging area\n","!git add \"Wav2Vec2forER KEX.ipynb\"  # Adjust to your notebook's name\n","\n","# Commit with a meaningful message\n","!git commit -m \"Updated Google Colab notebook\"\n","\n","# Push to GitHub\n","!git push origin main  # Push to 'main'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fk-2WbIOeH8X"},"outputs":[],"source":["%%capture\n","\n","!pip install git+https://github.com/huggingface/datasets.git\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install jiwer\n","!pip install torchaudio\n","!pip install librosa\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3BqgdEH6zK3"},"outputs":[],"source":["%env LC_ALL=C.UTF-8\n","%env LANG=C.UTF-8\n","%env TRANSFORMERS_CACHE=/content/cache\n","%env HF_DATASETS_CACHE=/content/cache\n","%env CUDA_LAUNCH_BLOCKING=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UMMxoDY2603t"},"outputs":[],"source":["# Monitor the training process\n","!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ypo6gHcL66jg"},"outputs":[],"source":["# # Uncomment this part if you want to setup your wandb project\n","from google.colab import userdata\n","import os\n","wandb_token = userdata.get(\"WANDB_TOKEN\")\n","\n","%env WANDB_WATCH=all\n","%env WANDB_LOG_MODEL=1\n","%env WANDB_PROJECT=Wav2Vec2forER\n","!wandb login {wandb_token} --relogin  # Use the secret for authentication\n"]},{"cell_type":"markdown","metadata":{"id":"yBODOxp37Mp3"},"source":["# Loading in and preparing the RAVDESS dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkHt3MKh6_GI"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# Set the custom cache directory to your new destination\n","import os\n","\n","# Re-load the dataset with the new cache\n","dataset = load_dataset(\"narad/ravdess\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EgsxxBkdz9rI"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import torchaudio\n","from sklearn.model_selection import train_test_split\n","\n","import os\n","import sys"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufKPt0d10NrS"},"outputs":[],"source":["import torchaudio\n","import librosa\n","import IPython.display as ipd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nK_Lx8qT0QQW"},"outputs":[],"source":["print(dataset['train'])"]},{"cell_type":"markdown","metadata":{"id":"riKDOTkl1mcz"},"source":["Creating label2id and id2label dictionaries to get easier overview of classes and labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8PEHcl6i1kZ5"},"outputs":[],"source":["# Get information about the dataset\n","print(dataset['train'].features)\n","label_names = dataset['train'].features['labels'].names\n","print(label_names)\n","\n","# Create a dictionary mapping label names to their corresponding IDs\n","label2id = {name: idx for idx, name in enumerate(label_names)}\n","\n","# Create a dictionary mapping label IDs to their corresponding label names\n","id2label = {idx: name for idx, name in enumerate(label_names)}\n","\n","# Print the dictionaries\n","print(\"Label to ID:\", label2id)\n","print(\"ID to Label:\", id2label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K47A0Jmo1R9u"},"outputs":[],"source":["df = dataset['train'].to_pandas()"]},{"cell_type":"markdown","metadata":{"id":"bUf5TOOU2rtq"},"source":["Adding an emotion column to the Dataframe to make things more clear."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xu6QUi72gvr"},"outputs":[],"source":["df[\"emotion\"] = df[\"labels\"].map(id2label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSOaOb0-2l_x"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"hlRnAMa92TvE"},"source":["Listening to a random sample:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RY0yv98B2TLE"},"outputs":[],"source":["idx = np.random.randint(0, len(df))\n","sample = df.iloc[idx]\n","\n","path = sample['audio'][\"path\"]\n","label = sample[\"emotion\"]\n","labelid = sample['labels']\n","\n","\n","print(f\"ID Location: {idx}\")\n","print(f\"      Label: {label}\")\n","print(f\"      Label: {labelid}\")\n","\n","print()\n","\n","speech, sr = torchaudio.load(path)\n","print(path)\n","print(speech[0])\n","speech = speech[0].numpy().squeeze()\n","print(speech)\n","speech = librosa.resample(y=speech, orig_sr=sr, target_sr=16000)  # Corrected usage\n","ipd.Audio(data=np.asarray(speech), autoplay=False, rate=16000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MRLV8F22qkW"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","import seaborn as sns\n","df.groupby('emotion').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n","plt.gca().spines[['top', 'right',]].set_visible(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eiMom77r3ONC"},"outputs":[],"source":["print(\"Labels: \", df[\"emotion\"].unique())\n","print()\n","df.groupby(\"emotion\").count()['audio']"]},{"cell_type":"markdown","metadata":{"id":"ATrs7aRk423c"},"source":["Restructuring the dataframe a bit for clarity:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4EHj8ue46u-"},"outputs":[],"source":["df[\"path\"] = df[\"audio\"].apply(lambda audio: audio.get(\"path\", None))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IuvOWeTm5eVy"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"nUXVqjfxG4jP"},"source":["Now we are gonna split the dataset into a train and test split and also save them into content/data as csv files:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rhKXt-8zG86Z"},"outputs":[],"source":["import os\n","\n","save_path = \"/content/data\"\n","\n","os.makedirs(save_path, exist_ok=True)\n","\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=101, stratify=df[\"labels\"])\n","\n","train_df = train_df.reset_index(drop=True)\n","test_df = test_df.reset_index(drop=True)\n","\n","train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n","test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n","\n","\n","print(train_df.shape)\n","print(test_df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDRCJxoQQRI_"},"outputs":[],"source":["# Loading the created dataset using datasets\n","from datasets import load_dataset, load_metric\n","\n","\n","data_files = {\n","    \"train\": \"/content/data/train.csv\",\n","    \"validation\": \"/content/data/test.csv\",\n","}\n","\n","dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\", )\n","print(dataset)\n","train_dataset = dataset[\"train\"]\n","eval_dataset = dataset[\"validation\"]\n","\n","print(train_dataset)\n","print(eval_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYvmIIa3e0P1"},"outputs":[],"source":["input_column = \"path\"\n","output_column = \"emotion\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeHS-R6CLMKE"},"outputs":[],"source":["# we need to distinguish the unique labels in our SER dataset\n","label_list = train_dataset.unique(output_column)\n","label_list.sort()  # Let's sort it for determinism\n","num_labels = len(label_list)\n","print(f\"A classification problem with {num_labels} classes: {label_list}\")"]},{"cell_type":"markdown","metadata":{"id":"H0kf6QKEJKLU"},"source":["# Loading in the Wav2Vec2 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMi2W7UjJT3B"},"outputs":[],"source":["from transformers import AutoConfig, Wav2Vec2Processor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Am17MhrnKiRC"},"outputs":[],"source":["model_name_or_path = \"facebook/wav2vec2-large-960h\"\n","pooling_mode = \"mean\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8aHVK5OLAi5"},"outputs":[],"source":["print(label2id)\n","print(id2label)\n","\n","# config\n","config = AutoConfig.from_pretrained(\n","    model_name_or_path,\n","    num_labels=len(label_list),\n","    label2id=label2id,\n","    id2label=id2label,\n","    finetuning_task=\"wav2vec2_clf\",\n",")\n","setattr(config, 'pooling_mode', pooling_mode)\n","print(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwnhVzMYsqy_"},"outputs":[],"source":["processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\n","print(processor)\n","target_sampling_rate = processor.feature_extractor.sampling_rate\n","print(f\"The target sampling rate: {target_sampling_rate}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JaL5OsJOSx4"},"outputs":[],"source":["def speech_file_to_array_fn(path):\n","    try:\n","        # Load and resample the audio\n","        speech_array, sampling_rate = torchaudio.load(path)\n","        resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n","        speech = resampler(speech_array)\n","        speech = speech.flatten()\n","\n","        # Convert to numpy array\n","        speech = speech.numpy()\n","\n","        if len(speech.shape) != 1:\n","            print(\"Hej\")\n","            raise ValueError(\"Expected a 1D numpy array of float values.\")\n","\n","    except Exception as e:\n","        print(f\"Error processing file {path}: {e}\")\n","        # Return a consistent placeholder (empty array)\n","        speech = np.array([])\n","\n","    return speech\n","\n","\n","\n","def label_to_id(label, label_list):\n","\n","    if len(label_list) > 0:\n","        return label_list.index(label) if label in label_list else -1\n","\n","    return label\n","\n","def preprocess_function(examples):\n","    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n","    target_list = [label2id[label] for label in examples[output_column]]\n","\n","    result = processor(speech_list, sampling_rate=target_sampling_rate)\n","    result[\"labels\"] = list(target_list)\n","\n","    return result"]},{"cell_type":"code","source":["train_dataset_copy = train_dataset\n","sample1 = speech_file_to_array_fn(train_dataset_copy[0]['path'])\n","print(sample1)\n","print(processor.feature_extractor)"],"metadata":{"id":"efu3SlJoOXQa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_sample_1 = processor(sample1, sampling_rate=target_sampling_rate)\n","print(processed_sample_1)"],"metadata":{"id":"-01L8QSHOn3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJS_Tc7MFmO7"},"outputs":[],"source":["print(len(train_dataset))\n","print(len(eval_dataset))\n","\n","\n","train_dataset = train_dataset.map(\n","    preprocess_function,\n","    batch_size=100,\n","    batched=True,\n","    num_proc=4\n",")\n","\n","eval_dataset = eval_dataset.map(\n","    preprocess_function,\n","    batch_size=100,\n","    batched=True,\n","    num_proc=4\n",")\n","\n","print(len(train_dataset))\n","print(len(eval_dataset))\n","print(train_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"blxz4xIbrg-F"},"outputs":[],"source":["idx = 0\n","print(f\"Training input_values: {train_dataset[idx]['input_values']}\")\n","print(f\"Training labels: {train_dataset[idx]['labels']} - {train_dataset[idx]['emotion']}\")"]},{"cell_type":"markdown","metadata":{"id":"4HigsAVYsWqK"},"source":["# Defining the model"]},{"cell_type":"markdown","metadata":{"id":"0_b1SLsEukV5"},"source":["Now we're going to create custom classes that define our model which will consist of the base wav2vec2 model + a classification head that succeeds the wav2vec2 model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nV2Mfqrpsafx"},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Optional, Tuple\n","import torch\n","from transformers.file_utils import ModelOutput\n","\n","\n","@dataclass\n","class SpeechClassifierOutput(ModelOutput):\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    attentions: Optional[Tuple[torch.FloatTensor]] = None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1xnqj56Kujqq"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","\n","from transformers.models.wav2vec2.modeling_wav2vec2 import (\n","    Wav2Vec2PreTrainedModel,\n","    Wav2Vec2Model,\n","    Wav2Vec2ForCTC\n",")\n","\n","\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    \"\"\"Head for wav2vec classification task.\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(config.final_dropout)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","\n","class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.pooling_mode = config.pooling_mode\n","        self.config = config\n","\n","        self.wav2vec2 = Wav2Vec2Model(config)\n","        self.classifier = Wav2Vec2ClassificationHead(config)\n","\n","        self.init_weights()\n","\n","    def freeze_feature_extractor(self):\n","        self.wav2vec2.feature_extractor._freeze_parameters()\n","\n","    def merged_strategy(\n","            self,\n","            hidden_states,\n","            mode=\"mean\"\n","    ):\n","        if mode == \"mean\":\n","            outputs = torch.mean(hidden_states, dim=1)\n","        elif mode == \"sum\":\n","            outputs = torch.sum(hidden_states, dim=1)\n","        elif mode == \"max\":\n","            outputs = torch.max(hidden_states, dim=1)[0]\n","        else:\n","            raise Exception(\n","                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n","\n","        return outputs\n","\n","    def forward(\n","            self,\n","            input_values,\n","            attention_mask=None,\n","            output_attentions=None,\n","            output_hidden_states=None,\n","            return_dict=None,\n","            labels=None,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","        outputs = self.wav2vec2(\n","            input_values,\n","            attention_mask=attention_mask,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        hidden_states = outputs[0]\n","        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n","        logits = self.classifier(hidden_states)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SpeechClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mrm-Koya8E-l"},"outputs":[],"source":["print(\"Columns in train_dataset:\", train_dataset.column_names)\n","# Display the first few examples\n","print(\"Sample data from train_dataset:\")\n","print(train_dataset[0:1])  # Adjust the slice to see more or fewer examples\n"]},{"cell_type":"markdown","metadata":{"id":"JatC_EJfcDCJ"},"source":["Cleaning up the datasets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KlSgz6hcCeJ"},"outputs":[],"source":["train_dataset = train_dataset.remove_columns(\"audio\")\n","eval_dataset = eval_dataset.remove_columns(\"audio\")\n","train_dataset = train_dataset.remove_columns(\"text\")\n","eval_dataset = eval_dataset.remove_columns(\"text\")\n","train_dataset = train_dataset.remove_columns(\"speaker_id\")\n","eval_dataset = eval_dataset.remove_columns(\"speaker_id\")\n","train_dataset = train_dataset.remove_columns(\"speaker_gender\")\n","eval_dataset = eval_dataset.remove_columns(\"speaker_gender\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"isgh1LQgcizh"},"outputs":[],"source":["print(\"Columns in train_dataset:\", train_dataset.column_names)\n","# Display the first few examples\n","print(\"Sample data from train_dataset:\")\n","print(train_dataset[0])  # Adjust the slice to see more or fewer examples\n"]},{"cell_type":"markdown","metadata":{"id":"YPPj2Pi0zlpm"},"source":["# Training!!"]},{"cell_type":"markdown","metadata":{"id":"tSpRO5a9zodW"},"source":["Now we will perform the final steps necessary and then start the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YQgH83dcc01"},"outputs":[],"source":["test_data = [\n","    {\"input_values\": list(range(10)), \"labels\": 0},\n","    {\"input_values\": list(range(20)), \"labels\": 1},\n","    {\"input_values\": list(range(15)), \"labels\": 2},\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gERGfK_Tzxsp"},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import Dict, List, Optional, Union\n","import torch\n","from transformers import Wav2Vec2Processor\n","\n","\n","@dataclass\n","class DataCollatorWithPadding:\n","    \"\"\"\n","    Data collator that pads only the input sequences, leaving the output labels unchanged.\n","    \"\"\"\n","\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","\n","    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n","        # Extract and pad only the input values\n","        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n","\n","        # Pad the input values with the given strategy and other specified options\n","        batch = self.processor.pad(\n","            input_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors=\"pt\",  # Return padded input as PyTorch tensors\n","        )\n","\n","        # Add labels to the batch without padding or modifications\n","        # Ensure the correct data type for labels\n","        label_features = [feature[\"labels\"] for feature in features]\n","        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n","\n","        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n","\n","\n","        return batch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CutUmJEogDyJ"},"outputs":[],"source":["data_collator = DataCollatorWithPadding(processor=processor, padding=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCX3J0cUdXrT"},"outputs":[],"source":["batch = data_collator(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CVEBcuugIKE"},"outputs":[],"source":["is_regression = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNRWxnYngKbg"},"outputs":[],"source":["import numpy as np\n","from transformers import EvalPrediction\n","\n","\n","def compute_metrics(p: EvalPrediction):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n","\n","    if is_regression:\n","        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n","    else:\n","        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"]},{"cell_type":"markdown","metadata":{"id":"T1YP4uWUg1e9"},"source":["Instantiating the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZLUwdqng1C9"},"outputs":[],"source":["model = Wav2Vec2ForSpeechClassification.from_pretrained(\n","    model_name_or_path,\n","    config=config,\n",")\n","print(config)"]},{"cell_type":"code","source":["import numpy as np\n","\n","def generate_sine_wave(freq, sample_rate, duration):\n","    t = np.linspace(0, duration, int(sample_rate * duration), False)  # Time axis\n","    waveform = np.sin(2 * np.pi * freq * t)  # Sine wave\n","    return waveform\n","\n","# Parameters\n","sample_rate = 16000  # 16 kHz sample rate, typical for audio processing\n","durations = [1.0, 0.5, 2.0]  # Durations in seconds\n","frequencies = [440, 1000, 250]  # Frequencies in Hz\n","\n","# Generate synthetic audio data\n","audio_samples = [generate_sine_wave(freq, sample_rate, duration)\n","                 for freq, duration in zip(frequencies, durations)]\n","print(audio_samples[0])\n"],"metadata":{"id":"SGRkDIjIMbTc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M6kJH6zkgoDz"},"outputs":[],"source":["model.freeze_feature_extractor()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPiWFyPkh5vj"},"outputs":[],"source":["%%capture\n","\n","!pip install accelerate -U\n","!pip install transformers[torch]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQYZgtdw650s"},"outputs":[],"source":["!pip show accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TZqH6KYhF3i"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","\"\"\"\n","training_args = TrainingArguments(\n","    output_dir=\"/content/wav2vec2-base-960h-RAVDESS\",\n","    per_device_train_batch_size=16,  # Increased from 4\n","    per_device_eval_batch_size=16,  # Increased from 4\n","    gradient_accumulation_steps=3,  # Adjusted to accumulate gradients more frequently\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=10.0,  # Increased to explore longer training\n","    fp16=True,\n","    save_steps=50,  # Increased to reduce I/O overhead\n","    eval_steps=50,  # Increased for consistent evaluation\n","    logging_steps=50,  # Adjusted for consistent logging\n","    learning_rate=3.5e-05,  # Intermediate learning rate\n","    save_total_limit=3,  # Increased limit for saved checkpoints\n",")\n","\"\"\"\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/content/wav2vec2-xlsr-greek-speech-emotion-recognition\",\n","    # output_dir=\"/content/MyDrive/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=2,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=3.0,\n","    fp16=True,\n","    save_steps=10,\n","    eval_steps=10,\n","    logging_steps=10,\n","    learning_rate=1e-4,\n","    save_total_limit=2,\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"aLvV-zlAlwdG"},"source":["OsÃ¤ker om denna ska vara kvar eller inte"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sU25zpnRjry-"},"outputs":[],"source":["from typing import Any, Dict, Union\n","import torch\n","from torch import nn\n","from transformers import Trainer\n","\n","class EmotionRecognitionTrainer(Trainer):\n","    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n","        \"\"\"\n","        Perform a training step on a batch of inputs.\n","\n","        Subclass and override to inject custom behavior.\n","\n","        Args:\n","            model (:obj:`nn.Module`):\n","                The model to train.\n","            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n","                The inputs and targets of the model.\n","\n","        Return:\n","            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n","        \"\"\"\n","\n","        model.train()\n","        inputs = self._prepare_inputs(inputs)\n","\n","        loss = self.compute_loss(model, inputs)  # Compute loss using the standard method\n","\n","        if self.args.gradient_accumulation_steps > 1:\n","            loss = loss / self.args.gradient_accumulation_steps\n","\n","        loss.backward()  # Perform backpropagation\n","\n","        return loss.detach()  # Return the loss for tracking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tInRpAMVlkDh"},"outputs":[],"source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=processor.feature_extractor,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z08Z1AFQlr5u"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","source":[],"metadata":{"id":"FOpP3IkzYbty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import librosa\n","from sklearn.metrics import classification_report"],"metadata":{"id":"BkAsx_RDWGN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IYxg1Tfo2VUw"},"source":["test_dataset = load_dataset(\"csv\", data_files={\"test\": \"/content/data/test.csv\"}, delimiter=\"\\t\")[\"test\"]\n","test_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")"],"metadata":{"id":"V9E1J2BcWjCq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1V-8tP2XYU3A"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPkAUdkAoMA/9r9ExJrVg6a"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}